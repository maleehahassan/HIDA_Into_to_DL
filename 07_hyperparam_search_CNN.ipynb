{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maleehahassan/HIDA_Into_to_DL/blob/main/07_hyperparam_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "1391c35148d8ed34"
      },
      "cell_type": "raw",
      "source": [
        "# Hyperparameter Search Demo\n",
        "\n",
        "This notebook demonstrates three different approaches to hyperparameter optimization:\n",
        "1. Grid Search - Exhaustive search through predefined parameter combinations\n",
        "2. Random Search - Random sampling from parameter distributions\n",
        "3. Optuna - Advanced optimization framework with adaptive sampling\n",
        "\n",
        "Key Concepts:\n",
        "- Hyperparameters are model configuration settings that aren't learned during training\n",
        "- Different search strategies offer trade-offs between computation time and optimization quality\n",
        "- We use a small CNN on CIFAR-10 for quick experimentation\n",
        "\n",
        "\n",
        "## 1. Setup and Imports\n",
        "\n",
        "This section sets up our environment with necessary libraries and ensures reproducibility.\n"
      ],
      "id": "1391c35148d8ed34"
    },
    {
      "metadata": {
        "id": "fa43734ca78e7dee"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Essential libraries for deep learning and data handling\n",
        "import time  # For measuring execution time\n",
        "import random  # For random parameter sampling\n",
        "import numpy as np  # For numerical operations\n",
        "import tensorflow as tf  # Main deep learning framework\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split  # For dataset splitting\n",
        "from itertools import product  # For generating parameter combinations\n",
        "\n",
        "# Optuna is optional - used for advanced hyperparameter optimization\n",
        "try:\n",
        "    import optuna\n",
        "    OPTUNA_AVAILABLE = True\n",
        "except Exception:\n",
        "    OPTUNA_AVAILABLE = False\n",
        "\n",
        "# Fix random seeds for reproducibility\n",
        "# This ensures we get the same results each time we run the notebook\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "tf.random.set_seed(seed)\n"
      ],
      "id": "fa43734ca78e7dee"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Preparation\n",
        "\n",
        "Here we prepare the CIFAR-10 dataset:\n",
        "- Load the full dataset (50,000 training + 10,000 test images)\n",
        "- Create a smaller subset for faster experimentation\n",
        "- Apply proper preprocessing and splitting"
      ],
      "metadata": {
        "id": "KagRw4_V9N7_"
      },
      "id": "KagRw4_V9N7_"
    },
    {
      "metadata": {
        "id": "9d9b46888801a0cf"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Create smaller subset for faster experimentation\n",
        "train_subset = 2000  # Instead of 50,000\n",
        "test_subset = 500    # Instead of 10,000\n",
        "x = np.concatenate([x_train, x_test])\n",
        "y = np.concatenate([y_train, y_test]).flatten()\n",
        "\n",
        "# Normalize pixel values to [0,1] range\n",
        "x = x.astype(\"float32\") / 255.0\n",
        "\n",
        "# Create stratified train/test split to maintain class distribution\n",
        "x_small, _, y_small, _ = train_test_split(\n",
        "    x, y,\n",
        "    train_size=train_subset + test_subset,\n",
        "    stratify=y,  # Ensure balanced classes\n",
        "    random_state=seed\n",
        ")\n",
        "x_train_small, x_test_small, y_train_small, y_test_small = train_test_split(\n",
        "    x_small, y_small,\n",
        "    train_size=train_subset,\n",
        "    stratify=y_small,\n",
        "    random_state=seed\n",
        ")"
      ],
      "id": "9d9b46888801a0cf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Model Definition\n",
        "\n",
        "Define a simple CNN architecture that we'll optimize:\n",
        "- 2 convolutional layers with max pooling\n",
        "- Flatten layer to connect to dense layers\n",
        "- Dense layer with dropout for regularization\n",
        "- Output layer for 10-class classification\n",
        "\n",
        "The model accepts several hyperparameters that we'll optimize:\n",
        "- conv_filters: Number of filters in conv layers\n",
        "- kernel_size: Size of conv filters\n",
        "- dense_units: Number of neurons in dense layer\n",
        "- dropout_rate: Dropout probability for regularization\n",
        "- learning_rate: Learning rate for Adam optimizer"
      ],
      "metadata": {
        "id": "LqBhX_wN9cD6"
      },
      "id": "LqBhX_wN9cD6"
    },
    {
      "metadata": {
        "id": "71c3b58dd45197e8"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "num_classes = 10  # CIFAR-10 has 10 classes\n",
        "input_shape = x_train_small.shape[1:]  # (32, 32, 3) for CIFAR-10\n",
        "\n",
        "def build_model(conv_filters=16, kernel_size=3, dense_units=64, dropout_rate=0.3, learning_rate=1e-3):\n",
        "    \"\"\"\n",
        "    Build and compile a CNN model with given hyperparameters.\n",
        "\n",
        "    Args:\n",
        "        conv_filters (int): Number of filters in first conv layer (doubled in second)\n",
        "        kernel_size (int): Size of convolutional filters\n",
        "        dense_units (int): Number of neurons in dense layer\n",
        "        dropout_rate (float): Dropout probability for regularization\n",
        "        learning_rate (float): Learning rate for Adam optimizer\n",
        "\n",
        "    Returns:\n",
        "        Compiled Keras model\n",
        "    \"\"\"\n",
        "    model = keras.Sequential()\n",
        "    # Input layer\n",
        "    model.add(layers.Input(shape=input_shape))\n",
        "    # First conv block\n",
        "    model.add(layers.Conv2D(conv_filters, kernel_size, activation='relu', padding='same'))\n",
        "    model.add(layers.MaxPooling2D())\n",
        "    # Second conv block\n",
        "    model.add(layers.Conv2D(conv_filters*2, kernel_size, activation='relu', padding='same'))\n",
        "    model.add(layers.MaxPooling2D())\n",
        "    # Flatten and dense layers\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(dense_units, activation='relu'))\n",
        "    model.add(layers.Dropout(dropout_rate))\n",
        "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, x_test, y_test):\n",
        "    \"\"\"Evaluate model accuracy on test data\"\"\"\n",
        "    loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "    return acc\n"
      ],
      "id": "71c3b58dd45197e8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Grid Search Implementation\n",
        "\n",
        "Grid Search:\n",
        "- Most basic approach to hyperparameter optimization\n",
        "- Tests every possible combination of given parameters\n",
        "- Guaranteed to find best combination in search space\n",
        "- Computationally expensive (scales exponentially with parameters)\n",
        "- May miss better values between grid points"
      ],
      "metadata": {
        "id": "ltXJD7oB9laf"
      },
      "id": "ltXJD7oB9laf"
    },
    {
      "metadata": {
        "id": "d87d98ebd80e7b39"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Training settings\n",
        "DEFAULT_EPOCHS = 3  # Keep small for demonstration\n",
        "DEFAULT_BATCH = 64\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'conv_filters': [8, 16],      # Number of conv filters\n",
        "    'dense_units': [32, 64],      # Neurons in dense layer\n",
        "    'dropout_rate': [0.2, 0.4],   # Dropout probabilities\n",
        "}\n",
        "\n",
        "print('\\nStarting simple manual grid search...')\n",
        "start = time.time()\n",
        "best_acc = 0.0\n",
        "best_params = None\n",
        "\n",
        "# Try every possible combination of parameters\n",
        "for conv_filters, dense_units, dropout_rate in product(\n",
        "    param_grid['conv_filters'],\n",
        "    param_grid['dense_units'],\n",
        "    param_grid['dropout_rate']\n",
        "):\n",
        "    # Create parameter dictionary for current combination\n",
        "    params = dict(conv_filters=conv_filters, dense_units=dense_units, dropout_rate=dropout_rate)\n",
        "    print('Testing params:', params)\n",
        "\n",
        "    # Build and train model with current parameters\n",
        "    model = build_model(**params)\n",
        "    model.fit(x_train_small, y_train_small, epochs=DEFAULT_EPOCHS, batch_size=DEFAULT_BATCH, verbose=0)\n",
        "\n",
        "    # Evaluate and update best if necessary\n",
        "    acc = evaluate_model(model, x_test_small, y_test_small)\n",
        "    print(' Accuracy:', acc)\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "        best_params = params\n",
        "\n",
        "print('Grid search done. Best accuracy: {:.4f} with {}'.format(best_acc, best_params))\n",
        "print('Time:', time.time() - start)\n"
      ],
      "id": "d87d98ebd80e7b39"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Random Search Implementation\n",
        "\n",
        "Random Search:\n",
        "- Randomly samples parameter combinations\n",
        "- More efficient than grid search for high-dimensional spaces\n",
        "- Can explore larger parameter ranges with fewer trials\n",
        "- May find better solutions by testing diverse combinations\n",
        "- Doesn't guarantee finding global optimum"
      ],
      "metadata": {
        "id": "0ynYE5nV9r6d"
      },
      "id": "0ynYE5nV9r6d"
    },
    {
      "metadata": {
        "id": "7fc8cd98548f35dc"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "print('\\nStarting random search...')\n",
        "start = time.time()\n",
        "best_acc_rs = 0.0\n",
        "best_params_rs = None\n",
        "\n",
        "# Define broader parameter distributions\n",
        "param_distributions = {\n",
        "    'conv_filters': [8, 16, 24],          # More options than grid search\n",
        "    'dense_units': [32, 64, 128],         # Wider range\n",
        "    'dropout_rate': [0.2, 0.3, 0.4],      # More granular\n",
        "    'learning_rate': [1e-3, 5e-4, 1e-4]   # Additional parameter\n",
        "}\n",
        "\n",
        "# Try random combinations\n",
        "n_iter = 4  # Number of random combinations to try\n",
        "for i in range(n_iter):\n",
        "    # Randomly sample one value from each parameter distribution\n",
        "    params = {k: random.choice(v) for k, v in param_distributions.items()}\n",
        "    print('Iter', i+1, 'params:', params)\n",
        "\n",
        "    # Build and train model with sampled parameters\n",
        "    model = build_model(**params)\n",
        "    model.fit(x_train_small, y_train_small, epochs=DEFAULT_EPOCHS, batch_size=DEFAULT_BATCH, verbose=0)\n",
        "\n",
        "    # Evaluate and update best if necessary\n",
        "    acc = evaluate_model(model, x_test_small, y_test_small)\n",
        "    print(' Accuracy:', acc)\n",
        "    if acc > best_acc_rs:\n",
        "        best_acc_rs = acc\n",
        "        best_params_rs = params\n",
        "\n",
        "print('Random search done. Best accuracy: {:.4f} with {}'.format(best_acc_rs, best_params_rs))\n",
        "print('Time:', time.time() - start)"
      ],
      "id": "7fc8cd98548f35dc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Optuna Implementation\n",
        "\n",
        "Optuna:\n",
        "- Advanced hyperparameter optimization framework\n",
        "- Supports various sampling strategies\n",
        "- Can adapt search based on previous results\n",
        "- Allows continuous parameter ranges\n",
        "- More efficient than grid or random search\n",
        "- Provides visualization and analysis tools\n"
      ],
      "metadata": {
        "id": "4HGiV7xD9yRX"
      },
      "id": "4HGiV7xD9yRX"
    },
    {
      "metadata": {
        "id": "eb01d633a82e0ce7"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "if OPTUNA_AVAILABLE:\n",
        "    print('\\nStarting Optuna optimization...')\n",
        "    start = time.time()\n",
        "\n",
        "    def objective(trial):\n",
        "        \"\"\"\n",
        "        Optuna objective function that:\n",
        "        1. Suggests parameter values using various sampling strategies\n",
        "        2. Builds and trains model with these parameters\n",
        "        3. Returns accuracy for Optuna to optimize\n",
        "        \"\"\"\n",
        "        # Define parameter space with different sampling strategies\n",
        "        conv = trial.suggest_categorical('conv_filters', [8, 16, 24])\n",
        "        dense = trial.suggest_categorical('dense_units', [32, 64, 128])\n",
        "        drop = trial.suggest_float('dropout_rate', 0.1, 0.5)  # Continuous range\n",
        "        lr = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)  # Log scale\n",
        "\n",
        "        # Build and train model\n",
        "        model = build_model(\n",
        "            conv_filters=conv,\n",
        "            dense_units=dense,\n",
        "            dropout_rate=drop,\n",
        "            learning_rate=lr\n",
        "        )\n",
        "        model.fit(x_train_small, y_train_small, epochs=DEFAULT_EPOCHS, batch_size=DEFAULT_BATCH, verbose=0)\n",
        "\n",
        "        # Return accuracy for optimization\n",
        "        acc = evaluate_model(model, x_test_small, y_test_small)\n",
        "        return acc\n",
        "\n",
        "    # Create and run Optuna study\n",
        "    study = optuna.create_study(direction='maximize')  # We want to maximize accuracy\n",
        "    study.optimize(objective, n_trials=4)  # Run 4 trials\n",
        "\n",
        "    # Report results\n",
        "    print('Optuna best value:', study.best_value)\n",
        "    print('Optuna best params:', study.best_params)\n",
        "    print('Time:', time.time() - start)\n",
        "else:\n",
        "    print('\\nOptuna not installed. Skipping Optuna section. To run Optuna, install it with:\\n    pip install optuna')\n",
        "\n",
        "print('\\nAll searches completed.')\n"
      ],
      "id": "eb01d633a82e0ce7"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
