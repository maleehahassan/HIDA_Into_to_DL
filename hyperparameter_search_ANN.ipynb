{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maleehahassan/HIDA_Into_to_DL/blob/main/hyperparameter_search_ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "f69671b4267d4bc8"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "\"\"\"\n",
        "hyperparameter_search_ANN.py\n",
        "\n",
        "Description:\n",
        "This script demonstrates simple hyperparameter tuning (grid search and random search)\n",
        "for a small Artificial Neural Network (ANN) on the CIFAR-10 dataset using TensorFlow/Keras.\n",
        "\n",
        "Why this is useful:\n",
        "- Hyperparameters (like learning rate, number of hidden units, dropout) strongly affect\n",
        "  model performance.\n",
        "- Grid search systematically explores combinations; random search samples combinations\n",
        "  randomly and can be more efficient when only a few hyperparameters matter.\n",
        "- This script is intentionally simple and uses manual loops so beginners can see the\n",
        "  tuning process without extra dependencies.\n",
        "\n",
        "Notes for beginners:\n",
        "- Keep epochs small for tuning to save time.\n",
        "- Use a validation split to compare hyperparameter settings.\n",
        "- After identifying good hyperparameters, retrain a final model with more epochs.\n",
        "\"\"\"\n",
        "\n",
        "# Simple and clear imports\n",
        "import itertools\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set random seeds for reproducibility (optional)\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# -------------------------\n",
        "# 1) Load and preprocess CIFAR-10\n",
        "# -------------------------\n",
        "# Load data\n",
        "(x_train_full, y_train_full), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Normalize pixels to range [0,1]\n",
        "x_train_full = x_train_full.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "# For a simple ANN, flatten images (32x32x3 -> 3072)\n",
        "x_train_full_flat = x_train_full.reshape((x_train_full.shape[0], -1))\n",
        "x_test_flat = x_test.reshape((x_test.shape[0], -1))\n",
        "\n",
        "# Convert labels to integers (already integers) and keep shape (n,)\n",
        "y_train_full = y_train_full.ravel()\n",
        "y_test = y_test.ravel()\n",
        "\n",
        "# Split a validation set from training data for hyperparameter search\n",
        "x_train, x_val, y_train, y_val = train_test_split(\n",
        "    x_train_full_flat, y_train_full, test_size=0.1, random_state=SEED, stratify=y_train_full\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# 2) Model builder function\n",
        "# -------------------------\n",
        "def create_model(input_shape, num_classes,\n",
        "                 hidden_units=512, dropout_rate=0.5,\n",
        "                 activation=\"relu\", learning_rate=1e-3):\n",
        "    \"\"\"\n",
        "    Build and compile a simple dense (fully-connected) network.\n",
        "    Arguments are hyperparameters we will tune.\n",
        "    \"\"\"\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Input(shape=(input_shape,)))\n",
        "    # First dense block\n",
        "    model.add(layers.Dense(hidden_units, activation=activation))\n",
        "    model.add(layers.Dropout(dropout_rate))\n",
        "    # Second dense block\n",
        "    model.add(layers.Dense(hidden_units // 2, activation=activation))\n",
        "    model.add(layers.Dropout(dropout_rate / 2))\n",
        "    # Output layer\n",
        "    model.add(layers.Dense(num_classes, activation=\"softmax\"))\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss=\"sparse_categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "# -------------------------\n",
        "# 3) Hyperparameter search configuration\n",
        "# -------------------------\n",
        "# Define grid for grid search (keep small for speed/demonstration)\n",
        "param_grid = {\n",
        "    \"hidden_units\": [256, 512],\n",
        "    \"dropout_rate\": [0.3, 0.5],\n",
        "    \"learning_rate\": [1e-3, 1e-4],\n",
        "    \"batch_size\": [64, 128],\n",
        "    \"activation\": [\"relu\"],\n",
        "}\n",
        "\n",
        "# Number of random samples for random search\n",
        "n_random_search = 6  # small number for demo; increase as needed\n",
        "\n",
        "# Number of epochs to train during the search (small for speed)\n",
        "search_epochs = 5\n",
        "\n",
        "# -------------------------\n",
        "# 4) Helper: evaluate one hyperparameter setting\n",
        "# -------------------------\n",
        "def evaluate_setting(params):\n",
        "    \"\"\"\n",
        "    Train model for a few epochs on the training set and return validation accuracy.\n",
        "    params: dict with keys hidden_units, dropout_rate, learning_rate, batch_size, activation\n",
        "    \"\"\"\n",
        "    model = create_model(\n",
        "        input_shape=x_train.shape[1],\n",
        "        num_classes=10,\n",
        "        hidden_units=params[\"hidden_units\"],\n",
        "        dropout_rate=params[\"dropout_rate\"],\n",
        "        activation=params[\"activation\"],\n",
        "        learning_rate=params[\"learning_rate\"],\n",
        "    )\n",
        "\n",
        "    # Verbose=0 to keep output tidy during many trainings\n",
        "    history = model.fit(\n",
        "        x_train, y_train,\n",
        "        validation_data=(x_val, y_val),\n",
        "        epochs=search_epochs,\n",
        "        batch_size=params[\"batch_size\"],\n",
        "        verbose=0,\n",
        "    )\n",
        "\n",
        "    # Get the best validation accuracy achieved during this run\n",
        "    val_acc = max(history.history[\"val_accuracy\"])\n",
        "    return val_acc\n",
        "\n",
        "# -------------------------\n",
        "# 5) Grid Search (manual)\n",
        "# -------------------------\n",
        "print(\"Starting manual grid search over {} combinations...\".format(\n",
        "    np.prod([len(v) for v in param_grid.values()])\n",
        "))\n",
        "\n",
        "# Create list of all combinations\n",
        "keys = list(param_grid.keys())\n",
        "all_combinations = list(itertools.product(*[param_grid[k] for k in keys]))\n",
        "\n",
        "grid_results = []\n",
        "for comb in all_combinations:\n",
        "    params = dict(zip(keys, comb))\n",
        "    print(\"Grid search trying:\", params)\n",
        "    val_acc = evaluate_setting(params)\n",
        "    print(f\" -> val_accuracy = {val_acc:.4f}\")\n",
        "    grid_results.append((val_acc, params))\n",
        "\n",
        "# Find best grid result\n",
        "best_grid_acc, best_grid_params = max(grid_results, key=lambda x: x[0])\n",
        "print(\"\\nBest grid search setting:\")\n",
        "print(best_grid_params)\n",
        "print(f\"Best validation accuracy (grid) = {best_grid_acc:.4f}\")\n",
        "\n",
        "# -------------------------\n",
        "# 6) Random Search (manual)\n",
        "# -------------------------\n",
        "print(\"\\nStarting manual random search ({} trials)...\".format(n_random_search))\n",
        "random_results = []\n",
        "# For random search, sample from ranges (here we sample from the grid lists for simplicity)\n",
        "for i in range(n_random_search):\n",
        "    params = {\n",
        "        \"hidden_units\": random.choice(param_grid[\"hidden_units\"]),\n",
        "        \"dropout_rate\": random.choice(param_grid[\"dropout_rate\"]),\n",
        "        \"learning_rate\": random.choice(param_grid[\"learning_rate\"]),\n",
        "        \"batch_size\": random.choice(param_grid[\"batch_size\"]),\n",
        "        \"activation\": random.choice(param_grid[\"activation\"]),\n",
        "    }\n",
        "    print(\"Random search trial {}: {}\".format(i + 1, params))\n",
        "    val_acc = evaluate_setting(params)\n",
        "    print(f\" -> val_accuracy = {val_acc:.4f}\")\n",
        "    random_results.append((val_acc, params))\n",
        "\n",
        "best_random_acc, best_random_params = max(random_results, key=lambda x: x[0])\n",
        "print(\"\\nBest random search setting:\")\n",
        "print(best_random_params)\n",
        "print(f\"Best validation accuracy (random) = {best_random_acc:.4f}\")\n",
        "\n",
        "# -------------------------\n",
        "# 7) Summary and suggestion for final training\n",
        "# -------------------------\n",
        "print(\"\\nSummary:\")\n",
        "print(f\"Grid best val_accuracy = {best_grid_acc:.4f} with params = {best_grid_params}\")\n",
        "print(f\"Random best val_accuracy = {best_random_acc:.4f} with params = {best_random_params}\")\n",
        "\n",
        "print(\"\\nSuggestion: retrain a final model with the best hyperparameters for more epochs (e.g., 30-100) and evaluate on the test set.\")\n",
        "\n",
        "# Example: quick final training with best grid params (brief)\n",
        "final_params = best_grid_params\n",
        "print(\"\\nQuick final training using best grid params (for demonstration)...\")\n",
        "final_model = create_model(\n",
        "    input_shape=x_train.shape[1],\n",
        "    num_classes=10,\n",
        "    hidden_units=final_params[\"hidden_units\"],\n",
        "    dropout_rate=final_params[\"dropout_rate\"],\n",
        "    activation=final_params[\"activation\"],\n",
        "    learning_rate=final_params[\"learning_rate\"],\n",
        ")\n",
        "final_model.fit(\n",
        "    np.concatenate([x_train, x_val]),  # use both train+val for final training\n",
        "    np.concatenate([y_train, y_val]),\n",
        "    epochs=10,  # keep small here; increase in real use\n",
        "    batch_size=final_params[\"batch_size\"],\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "test_loss, test_acc = final_model.evaluate(x_test_flat, y_test, verbose=0)\n",
        "print(f\"Test accuracy of final model (demo) = {test_acc:.4f}\")\n",
        "\n"
      ],
      "id": "f69671b4267d4bc8"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}