{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8335d5f6",
   "metadata": {},
   "source": [
    "#  ðŸ§  A Neural Network for the Iris Dataset\n",
    "# \n",
    "# Welcome! In this notebook, we'll build a neural network to classify the famous **Iris dataset**. We will still use **only NumPy**.\n",
    "\n",
    "## This introduces us to **multi-class classification**, which requires a few new tools:\n",
    "\n",
    "# * **One-Hot Encoding:** Converting integer labels (0, 1, 2) into vectors (`[1,0,0]`, `[0,1,0]`, `[0,0,1]`).\n",
    "# * **Softmax Activation:** Used in the output layer to create probabilities for multiple classes that sum to 1.0.\n",
    "# * **Cross-Entropy Loss:** The standard way to measure error for multi-class probabilities (better than MSE here).\n",
    "# * **Data Scaling:** Normalizing input data to help the network learn faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7d63386",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6ebe814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fa2d2b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "830e8727",
   "metadata": {},
   "source": [
    "# Step 1: Load and Preprocess Data\n",
    "#\n",
    "# Neural networks work best when input data is small (usually between 0 and 1) and centered.\n",
    "#\n",
    "# We need two manual preprocessing functions:\n",
    "# 1.  **`to_one_hot`**: Converts labels `0` -> `[1, 0, 0]`.\n",
    "# 2.  **`min_max_scale`**: Squashes all input features to be between 0 and 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3da51a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data \n",
    "iris = load_iris()\n",
    "X_raw = iris.data      # (150 samples, 4 features)\n",
    "y_raw = iris.target    # (150 samples,) containing 0, 1, 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d80afeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Helper Functions \n",
    "\n",
    "def to_one_hot(y, num_classes):\n",
    "    \"\"\"Converts (150,) array of ints to (150, 3) one-hot matrix\"\"\"\n",
    "    one_hot = np.zeros((y.shape[0], num_classes))\n",
    "    for i, label in enumerate(y):\n",
    "        one_hot[i, label] = 1.0\n",
    "    return one_hot\n",
    "\n",
    "def min_max_scale(X):\n",
    "    \"\"\"Scales features to be between 0 and 1\"\"\"\n",
    "    min_val = X.min(axis=0)\n",
    "    max_val = X.max(axis=0)\n",
    "    return (X - min_val) / (max_val - min_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d9c9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Data\n",
    "X = #yourcode         # Scale features to [0, 1]\n",
    "\n",
    "# One-hot encode the outputs\n",
    "num_classes = #yourcode\n",
    "y = #yourcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67fb8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data (important because Iris is sorted by class!)\n",
    "indices = np.arange(X.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "X = X[indices]\n",
    "y = y[indices]\n",
    "\n",
    "print(\"First 5 X samples (scaled):\\n\", X[:5])\n",
    "print(\"\\nFirst 5 y samples (one-hot):\\n\", y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6cec37",
   "metadata": {},
   "source": [
    "# Step 2: Advanced Activation & Loss\n",
    "#\n",
    "# Softmax (for Output)\n",
    "# Sigmoid squashes *independently* between 0 and 1. Softmax squashes *relative to other classes* so they all sum to 1.\n",
    "# Formula: $Softmax(z_i) = \\frac{e^{z_i}}{\\sum e^{z_j}}$\n",
    "#\n",
    "# Cross-Entropy (The Loss)\n",
    "# This heavily penalizes confident wrong answers.\n",
    "#\n",
    "# The Magic Shortcut\n",
    "# The derivative calculation for Softmax + Cross-Entropy combined is incredibly simple:\n",
    "# `Gradient = Predicted_Probabilities - True_One_Hot_Labels`\n",
    "#\n",
    "# This is why we often pair them together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9906d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return #yourcode\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def softmax(x):\n",
    "    # Subtract max for numerical stability (prevents blowing up exp)\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b89981d",
   "metadata": {},
   "source": [
    "# Step 3: Network Architecture\n",
    "#\n",
    "# * **Inputs:** 4 (Sepal Length, Sepal Width, Petal Length, Petal Width)\n",
    "# * **Hidden:** 6 (Add complexity)\n",
    "# * **Output:** 3 (One for each Iris species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee753a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_neurons = \n",
    "hidden_neurons = \n",
    "output_neurons = \n",
    "\n",
    "learning_rate = \n",
    "epochs =\n",
    "\n",
    "# Initialize weights (standard normal distribution usually works better here)\n",
    "# We multiply by 0.1 to keep initial weights small\n",
    "\n",
    "weights_hidden = np.random.randn(input_neurons, hidden_neurons) * 0.1\n",
    "bias_hidden = np.zeros((1, hidden_neurons))\n",
    "\n",
    "weights_output = np.random.randn(hidden_neurons, output_neurons) * 0.1\n",
    "bias_output = np.zeros((1, output_neurons))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4473dd",
   "metadata": {},
   "source": [
    "# Step 4: The Training Loop\n",
    "#\n",
    "# notice the **Backward Pass** for the output layer uses our \"magic shortcut\": `predicted_output - y`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ace738",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    # 1. FORWARD PASS \n",
    "    hidden_layer_input = np.dot(X, weights_hidden) + bias_hidden\n",
    "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "\n",
    "    output_layer_input = np.dot(hidden_layer_output, weights_output) + bias_output\n",
    "    # USE SOFTMAX NOW\n",
    "    predicted_output = softmax(output_layer_input)\n",
    "\n",
    "    # 2. LOSS (Cross-Entropy for monitoring)\n",
    "    # Small epsilon to prevent log(0) errors\n",
    "    epsilon = 1e-15\n",
    "    clipped_preds = np.clip(predicted_output, epsilon, 1 - epsilon)\n",
    "    loss = -np.mean(np.sum(y * np.log(clipped_preds), axis=1))\n",
    "    loss_history.append(loss)\n",
    "\n",
    "    # 3. BACKWARD PASS\n",
    "    # Gradient of Cross-Entropy + Softmax is just (Pred - Actual)\n",
    "    d_predicted_output = (predicted_output - y) / X.shape[0] # Normalize by batch size\n",
    "\n",
    "    # Backprop to hidden layer\n",
    "    error_hidden = d_predicted_output.dot(weights_output.T)\n",
    "    d_hidden_layer = error_hidden * sigmoid_derivative(hidden_layer_output)\n",
    "\n",
    "    # 4. UPDATE WEIGHTS \n",
    "    weights_output -= hidden_layer_output.T.dot(d_predicted_output) * learning_rate\n",
    "    bias_output -= np.sum(d_predicted_output, axis=0, keepdims=True) * learning_rate\n",
    "    weights_hidden -= X.T.dot(d_hidden_layer) * learning_rate\n",
    "    bias_hidden -= np.sum(d_hidden_layer, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "    if i % 500 == 0:\n",
    "        print(f\"Epoch {i} Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad374d5",
   "metadata": {},
   "source": [
    "# Step 5: Testing Accuracy\n",
    "\n",
    "# To check accuracy, we convert our one-hot predictions back into single integers using `np.argmax` (which finds the index of the highest probability).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165bd681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final forward pass to get predictions\n",
    "hidden_out = sigmoid(np.dot(X, weights_hidden) + bias_hidden)\n",
    "final_preds_prob = softmax(np.dot(hidden_out, weights_output) + bias_output)\n",
    "\n",
    "# Convert probabilities to class labels (0, 1, or 2)\n",
    "predicted_classes = np.argmax(final_preds_prob, axis=1)\n",
    "true_classes = np.argmax(y, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(predicted_classes == true_classes)\n",
    "print(f\"\\nFinal Training Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Show a few examples\n",
    "print(\"\\nSample Predictions vs True Labels:\")\n",
    "print(f\"Predicted: {predicted_classes[:10]}\")\n",
    "print(f\"True:      {true_classes[:10]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
