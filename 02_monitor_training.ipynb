{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maleehahassan/HIDA_Into_to_DL/blob/main/02_monitor_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview: Predicting Sunshine Hours Tomorrow with Neural Networks\n",
        "\n",
        "This notebook demonstrates a regression task using a weather prediction dataset. The goal is to predict the number of sunshine hours in Basel for the next day using historical weather data and neural networks. The workflow includes:\n",
        "\n",
        "- **Data Loading & Exploration**: The weather dataset is loaded from a remote CSV file. Summary statistics, shape, and column names are explored to understand the data structure.\n",
        "- **Feature Selection**: Features are selected by dropping date-related columns. The target variable is set as the next day's sunshine hours in Basel.\n",
        "- **Data Splitting**: The data is split into training, validation, and test sets to enable model evaluation and prevent overfitting.\n",
        "- **Model Building**: A feedforward neural network is constructed using TensorFlow/Keras, with customizable hidden layers for regression.\n",
        "- **Training & Monitoring**: The model is trained for multiple epochs, and training progress is monitored using RMSE (Root Mean Squared Error).\n",
        "- **Evaluation & Visualization**: Model predictions are compared to true values using scatter plots and RMSE metrics. Overfitting is discussed and addressed by adjusting model complexity and using early stopping.\n",
        "- **Baseline Comparison**: A simple baseline predictor (yesterday's sunshine hours) is used for comparison to assess model performance.\n",
        "- **Further Techniques**: Suggestions for additional improvements, such as batch normalization and dropout layers, are provided.\n",
        "\n",
        "This notebook guides learners through the process of preparing data, building and training a neural network for regression, evaluating results, and improving model generalization on a real-world weather dataset."
      ],
      "metadata": {
        "id": "BYuD8JKVaHo0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Mxscvp8OnXO"
      },
      "source": [
        "# Predicting the Sunshine Hours Tomorrow\n",
        "\n",
        "A regression task on the weather prediction dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvGfotamFy-I",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Import pandas for data manipulation\n",
        "import pandas as pd\n",
        "\n",
        "# Load the weather prediction dataset from a remote CSV file\n",
        "df = pd.read_csv(\"https://zenodo.org/record/5071376/files/weather_prediction_dataset.csv\")\n",
        "\n",
        "# Display summary statistics of the dataframe\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zm_gLKs7IGL7",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Show the shape (rows, columns) of the dataframe\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRPhFa2DIQFO",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# List all column names in the dataframe\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3SpohHeITIr",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Print the set of unique suffixes in column names (excluding 'MONTH' and 'DATE')\n",
        "print(set({ \"_\".join(x.split(\"_\")[1:]) for x in df.columns if x not in [\"MONTH\", \"DATE\"]  }))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VS4wxvmI1SZ"
      },
      "outputs": [],
      "source": [
        "# Set the number of rows to use (3 years of daily data)\n",
        "nr_rows = 365*3\n",
        "\n",
        "# Select the feature columns, dropping 'DATE' and 'MONTH'\n",
        "X_data = df.loc[:nr_rows].drop(columns=[\"DATE\", \"MONTH\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9agOYJiJR4h",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Show the shape of the feature matrix\n",
        "X_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJDQ7DxdJS7w",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Print all column names that contain 'BASEL'\n",
        "print([ item for item in df.columns if \"BASEL\" in item])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3gavAQpJfij"
      },
      "outputs": [],
      "source": [
        "# Select the target variable: next day's sunshine hours in Basel\n",
        "y_data = df.loc[1:(nr_rows+1)][\"BASEL_sunshine\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-58cfGxdJtVE",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Show the data type of the target variable\n",
        "y_data.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QgkzJpyJvI1"
      },
      "outputs": [],
      "source": [
        "# Import train_test_split for splitting data into train/validation/test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and not-training sets (70% train, 30% not-train)\n",
        "X_train, X_not_train, y_train, y_not_train = train_test_split(X_data, y_data, test_size=.3, random_state=20211013)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwqbmsGYKLfP"
      },
      "outputs": [],
      "source": [
        "# Split the not-training set into validation and test sets (each 15% of total)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_not_train, y_not_train, test_size=.5, random_state=20211014)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDmGw9-qKaW2",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Print the shape of the training feature matrix\n",
        "print(X_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ondX5Dg1Kci7"
      },
      "outputs": [],
      "source": [
        "# Print the shapes of the test and validation feature matrices\n",
        "print(X_test.shape, X_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acaHVVj3KfIg"
      },
      "outputs": [],
      "source": [
        "# Import keras for building neural networks\n",
        "from tensorflow import keras\n",
        "\n",
        "# Define a function to create a neural network model\n",
        "def create_nn():\n",
        "\n",
        "  inputs = keras.Input(shape=(X_data.shape[1],), name=\"input\")\n",
        "\n",
        "  dense1 = keras.layers.Dense(100, 'relu', name=\"dense1\")(inputs)\n",
        "  dense2 = keras.layers.Dense(50, 'relu', name=\"dense2\")(dense1)\n",
        "\n",
        "  outputs = keras.layers.Dense(1)(dense2)\n",
        "\n",
        "  return keras.Model(inputs=inputs, outputs=outputs, name=\"weather_prediction_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxSNOPQLLQgu"
      },
      "outputs": [],
      "source": [
        "# Create the neural network model\n",
        "model = create_nn()\n",
        "\n",
        "# Display the model architecture\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPd7U1V-LUb5"
      },
      "outputs": [],
      "source": [
        "# Compile the model with Adam optimizer, mean squared error loss, and RMSE metric\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=\"mse\", #MEAN SQUARED ERROR\n",
        "              metrics=[keras.metrics.RootMeanSquaredError()])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dagh13EBLyKa"
      },
      "outputs": [],
      "source": [
        "# Train the model on the training data for 200 epochs\n",
        "history = model.fit(X_train, y_train,\n",
        "                    batch_size = 32,\n",
        "                    epochs=200,\n",
        "                    verbose=2\n",
        "                    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7GZ_rUnMBmh"
      },
      "outputs": [],
      "source": [
        "# Import seaborn and matplotlib for plotting\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert the training history to a DataFrame for analysis\n",
        "history_df = pd.DataFrame.from_dict(history.history)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOE9GiUGMVs-"
      },
      "outputs": [],
      "source": [
        "# List all columns in the history DataFrame\n",
        "history_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmtKu1eZMW_C"
      },
      "outputs": [],
      "source": [
        "# Plot the RMSE on the training data over epochs\n",
        "sns.lineplot(data=history_df['root_mean_squared_error'])\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"RMSE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIwccIedMioy"
      },
      "outputs": [],
      "source": [
        "# Make predictions on the training and test sets\n",
        "y_train_predict = model.predict(X_train)\n",
        "y_test_predict = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRPYVXcgMzIx"
      },
      "outputs": [],
      "source": [
        "# Create scatter plots to compare predicted and true sunshine hours\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "axes[0].scatter(y_train_predict, y_train, s=10, alpha=0.5, color=\"teal\")\n",
        "axes[0].set_title(\"training set\")\n",
        "axes[0].set_xlabel(\"predicted sunshine hours\")\n",
        "axes[0].set_ylabel(\"true sunshine hours\")\n",
        "\n",
        "axes[1].scatter(y_test_predict, y_test, s=10, alpha=0.5, color=\"teal\")\n",
        "axes[1].set_title(\"test set\")\n",
        "axes[1].set_xlabel(\"predicted sunshine hours\")\n",
        "axes[1].set_ylabel(\"true sunshine hours\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdXG6u2vNEXJ"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model performance on the training and test sets\n",
        "loss_train, rmse_train = model.evaluate(X_train, y_train)\n",
        "loss_test, rmse_test = model.evaluate(X_test, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQNnuyOMNnmb"
      },
      "outputs": [],
      "source": [
        "# Print the RMSE for the training and test sets\n",
        "print(\"training set\",rmse_train)\n",
        "print(\"    test set\",rmse_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e25VTTySObcu"
      },
      "source": [
        "# Yikes Overfitting !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AR2Za-mEPRaj"
      },
      "source": [
        "## Set expectations: How difficult is the defined problem?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTjnb_W-OfDq"
      },
      "outputs": [],
      "source": [
        "# Use yesterday's sunshine hours as a simple baseline predictor\n",
        "y_baseline_prediction = X_test[\"BASEL_sunshine\"]\n",
        "\n",
        "plt.figure(figsize=(5,5), dpi=100)\n",
        "plt.scatter(y_baseline_prediction, y_test, s=10, alpha=.5)\n",
        "plt.xlabel(\"sunshine hours yesterday (baseline)\")\n",
        "plt.ylabel(\"true sunshine hours\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import mean_squared_error for performance comparison\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Calculate the RMSE of the neural network predictions\n",
        "mse_nn = mean_squared_error(y_test, y_test_predict)\n",
        "rmse_nn = np.sqrt(mse_nn)\n",
        "\n",
        "# Calculate the RMSE of the baseline predictions\n",
        "mse_baseline = mean_squared_error(y_test, y_baseline_prediction)\n",
        "rmse_baseline = np.sqrt(mse_baseline)\n"
      ],
      "metadata": {
        "id": "HXOAmcvPJC7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdLgF6jpP5EN"
      },
      "outputs": [],
      "source": [
        "# Import mean_squared_error for performance comparison\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Calculate the RMSE of the neural network predictions\n",
        "rmse_nn = mean_squared_error(y_test, y_test_predict)\n",
        "\n",
        "\n",
        "# Calculate the RMSE of the baseline predictions\n",
        "rmse_baseline = mean_squared_error(y_test, y_baseline_prediction)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgeClXkQQYuU"
      },
      "outputs": [],
      "source": [
        "# Print the RMSE for the neural network and baseline models\n",
        "print(\"training set\",rmse_nn)\n",
        "print(\"baseline set\",rmse_baseline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMqX6nFNQfJl"
      },
      "outputs": [],
      "source": [
        "# Recreate and compile the neural network model\n",
        "model = create_nn()\n",
        "model.compile(optimizer='adam',\n",
        "              loss='mse',\n",
        "              metrics=[keras.metrics.RootMeanSquaredError()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGX2St64QzbS"
      },
      "outputs": [],
      "source": [
        "# Train the model on the training data for 200 epochs, with validation on the validation set\n",
        "history = model.fit(X_train, y_train,\n",
        "                    batch_size=32,\n",
        "                    epochs=200,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8Uakud8REk5"
      },
      "outputs": [],
      "source": [
        "# Convert the training history to a DataFrame for analysis\n",
        "history_df = pd.DataFrame.from_dict(history.history)\n",
        "\n",
        "# Plot the RMSE on the training and validation data over epochs\n",
        "sns.lineplot(data=history_df[['root_mean_squared_error','val_root_mean_squared_error']])\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"RMSE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpQYGUz4R0Uz"
      },
      "source": [
        "## Counteract model overfitting\n",
        "\n",
        "reduce the number of parameters of our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnIZe-3qRgPk"
      },
      "outputs": [],
      "source": [
        "# Define a new create_nn function that takes the number of nodes as arguments\n",
        "def create_nn(nodes1, nodes2):\n",
        "\n",
        "  inputs = keras.Input(shape=(X_data.shape[1],), name=\"input\")\n",
        "\n",
        "  dense1 = keras.layers.Dense(nodes1, 'relu', name=\"dense1\")(inputs)\n",
        "  dense2 = keras.layers.Dense(nodes2, 'relu', name=\"dense2\")(dense1)\n",
        "\n",
        "  outputs = keras.layers.Dense(1)(dense2)\n",
        "\n",
        "  return keras.Model(inputs=inputs, outputs=outputs, name=\"weather_prediction_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxNUcemXSOT0"
      },
      "outputs": [],
      "source": [
        "# Create a smaller neural network model with 10 and 5 nodes in the hidden layers\n",
        "model = create_nn(10,5)\n",
        "\n",
        "# Display the model architecture\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jRz4sTAScKX"
      },
      "outputs": [],
      "source": [
        "# Compile the model with Adam optimizer, mean squared error loss, and RMSE metric\n",
        "model.compile(optimizer='adam',\n",
        "              loss='mse',\n",
        "              metrics=[keras.metrics.RootMeanSquaredError()])\n",
        "\n",
        "# Train the model on the training data for 200 epochs, with validation on the validation set\n",
        "history = model.fit(X_train, y_train,\n",
        "                    batch_size = 32,\n",
        "                    epochs = 200,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    verbose = 2)\n",
        "\n",
        "# Convert the training history to a DataFrame for analysis\n",
        "history_df = pd.DataFrame.from_dict(history.history)\n",
        "\n",
        "# Plot the RMSE on the training and validation data over epochs\n",
        "sns.lineplot(data=history_df[['root_mean_squared_error', 'val_root_mean_squared_error']])\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"RMSE\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRjAlQ5ZSkYc"
      },
      "outputs": [],
      "source": [
        "# Create a larger neural network model with 100 and 50 nodes in the hidden layers\n",
        "model = create_nn(100, 50)\n",
        "\n",
        "# Compile the model with Adam optimizer, mean squared error loss, and RMSE metric\n",
        "model.compile(optimizer='adam',\n",
        "              loss='mse',\n",
        "              metrics=[keras.metrics.RootMeanSquaredError()])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICPofG62TBFr"
      },
      "outputs": [],
      "source": [
        "# Import EarlyStopping to prevent overfitting\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "# Set up EarlyStopping to monitor validation loss and stop training if it doesn't improve for 10 epochs\n",
        "earlystop = EarlyStopping(monitor='val_loss',\n",
        "                          patience=10,verbose=1)\n",
        "\n",
        "# Train the model on the training data for 200 epochs, with validation on the validation set\n",
        "# EarlyStopping will halt training early if there's no improvement\n",
        "history = model.fit(X_train, y_train,\n",
        "                    batch_size = 32,\n",
        "                    epochs=200,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    callbacks=[earlystop],\n",
        "                    verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXl_vL9TTnIz"
      },
      "outputs": [],
      "source": [
        "# Convert the training history to a DataFrame for analysis\n",
        "history_df = pd.DataFrame.from_dict(history.history)\n",
        "\n",
        "# Plot the RMSE on the training and validation data over epochs\n",
        "sns.lineplot(data=history_df[['root_mean_squared_error', 'val_root_mean_squared_error']])\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"RMSE\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnwuUnI8ULWH"
      },
      "source": [
        "Further  techniques of interest:\n",
        "- batchnormalisation\n",
        "- dropout layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhqeosVpTs3Z"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}