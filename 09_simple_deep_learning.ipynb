{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maleehahassan/HIDA_Into_to_DL/blob/main/09_simple_deep_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "df60d5ed47417df3"
      },
      "cell_type": "markdown",
      "source": [
        "# Simple Deep Learning with PyTorch\n",
        "\n",
        "This notebook implements a basic deep learning model using PyTorch to classify MNIST digits. We'll cover:\n",
        "- Building a simple neural network\n",
        "- Training and validation process\n",
        "- Performance visualization\n",
        "\n",
        "## Step 1: Import Required Libraries\n",
        "\n",
        "We need the following libraries:\n",
        "- torch: Main PyTorch library\n",
        "- torch.nn: Neural network modules\n",
        "- torch.optim: Optimization algorithms\n",
        "- torchvision: For accessing the MNIST dataset\n",
        "- matplotlib: For visualization"
      ],
      "id": "df60d5ed47417df3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b07028f1",
      "metadata": {
        "id": "b07028f1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "metadata": {
        "id": "890a6786c6f07814"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 2: Set Random Seed\n",
        "\n",
        "Setting a random seed ensures reproducible results across different runs."
      ],
      "id": "890a6786c6f07814"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e77dca7e",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "e77dca7e"
      },
      "outputs": [],
      "source": [
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "metadata": {
        "id": "e71f7cf518d2de95"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 3: Define Neural Network Architecture\n",
        "\n",
        "Create a simple neural network with:\n",
        "1. Input layer: 784 neurons (28x28 MNIST images flattened)\n",
        "2. Hidden layer: 128 neurons with ReLU activation\n",
        "3. Output layer: 10 neurons (one for each digit)\n",
        "\n",
        "The network includes:\n",
        "- Flattening operation to convert 2D images to 1D\n",
        "- Fully connected layers\n",
        "- ReLU activation for non-linearity"
      ],
      "id": "e71f7cf518d2de95"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41341685",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "41341685"
      },
      "outputs": [],
      "source": [
        "# Define the neural network\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        # Input layer (28x28 = 784 pixels) -> Hidden layer (128 neurons) -> Output layer (10 classes)\n",
        "        self.flatten = nn.Flatten()  # Flatten the 28x28 image to 784 pixels\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(784, 128),     # First layer: 784 -> 128\n",
        "            nn.ReLU(),               # Activation function\n",
        "            nn.Linear(128, 10)       # Output layer: 128 -> 10 (number of classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)          # Forward propagation\n",
        "        return self.layers(x)"
      ]
    },
    {
      "metadata": {
        "id": "32c5238cfa124f7c"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 4: Define Training Function\n",
        "\n",
        "The `train_model` function handles:\n",
        "1. Training phase:\n",
        "   - Forward propagation\n",
        "   - Loss computation\n",
        "   - Backward propagation\n",
        "   - Weight updates\n",
        "2. Validation phase:\n",
        "   - Model evaluation\n",
        "   - Metrics calculation\n",
        "\n",
        "It tracks:\n",
        "- Loss values\n",
        "- Accuracy metrics\n",
        "- Training progress"
      ],
      "id": "32c5238cfa124f7c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ceaed407",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "ceaed407"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs):\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accuracies, val_accuracies = [], []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()           # puts model in training mode\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()     # Clear gradients\n",
        "            loss.backward()          # Compute gradients\n",
        "            optimizer.step()         # Update weights\n",
        "\n",
        "            # Calculate training statistics\n",
        "            running_loss += loss.item()                     # add up loss\n",
        "            _, predicted = torch.max(outputs.data, 1)       # get predicted classes\n",
        "            total += labels.size(0)                         # count how many images seen\n",
        "            correct += (predicted == labels).sum().item()   # count correct ones\n",
        "\n",
        "        # Calculate training metrics\n",
        "        train_loss = running_loss / len(train_loader)           # Average loss over all training batches\n",
        "        train_accuracy = 100 * correct / total                  # Accuracy (%) = (number of correct predictions / total samples) * 100\n",
        "\n",
        "        # Save the results so we can plot them later\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()     # puts model in evaluation mode\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():  # No need to compute gradients during validation\n",
        "            for images, labels in val_loader:\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Calculate validation metrics\n",
        "        val_loss = val_loss / len(val_loader)\n",
        "        val_accuracy = 100 * correct / total\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}]:')\n",
        "        print(f'Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%')\n",
        "        print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
        "        print('-' * 60)\n",
        "\n",
        "    return train_losses, val_losses, train_accuracies, val_accuracies"
      ]
    },
    {
      "metadata": {
        "id": "e57bcf4a4101cfb5"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 5: Define Visualization Function\n",
        "\n",
        "The `plot_metrics` function creates two plots:\n",
        "1. Training and Validation Loss\n",
        "2. Training and Validation Accuracy\n",
        "\n",
        "This helps visualize:\n",
        "- Model convergence\n",
        "- Potential overfitting\n",
        "- Learning progress"
      ],
      "id": "e57bcf4a4101cfb5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd7da1cd",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "dd7da1cd"
      },
      "outputs": [],
      "source": [
        "def plot_metrics(train_losses, val_losses, train_accuracies, val_accuracies):\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "    # Create a figure with two subplots\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Plot losses\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, train_losses, 'b-', label='Training Loss')\n",
        "    plt.plot(epochs, val_losses, 'r-', label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot accuracies\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, train_accuracies, 'b-', label='Training Accuracy')\n",
        "    plt.plot(epochs, val_accuracies, 'r-', label='Validation Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "metadata": {
        "id": "7a155e2fc5f87c5f"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 6: Main Execution Function\n",
        "\n",
        "The `main` function orchestrates the entire process:\n",
        "\n",
        "1. Hyperparameter setup:\n",
        "   - Batch size: 64\n",
        "   - Learning rate: 0.001\n",
        "   - Number of epochs: 10\n",
        "\n",
        "2. Data preparation:\n",
        "   - Load MNIST dataset\n",
        "   - Apply transformations\n",
        "   - Split into train/validation sets\n",
        "\n",
        "3. Model setup:\n",
        "   - Initialize neural network\n",
        "   - Define loss function\n",
        "   - Configure optimizer\n",
        "\n",
        "4. Training and visualization:\n",
        "   - Train the model\n",
        "   - Plot performance metrics"
      ],
      "id": "7a155e2fc5f87c5f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb12b2e9",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "fb12b2e9"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    # Hyperparameters\n",
        "    batch_size = 64\n",
        "    learning_rate = 0.001\n",
        "    num_epochs = 10\n",
        "\n",
        "    # Data preprocessing\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n",
        "    ])\n",
        "\n",
        "    # Load MNIST dataset\n",
        "    full_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "\n",
        "    # Split into training and validation sets (80% train, 20% validation)\n",
        "    train_size = int(0.8 * len(full_dataset))\n",
        "    val_size = len(full_dataset) - train_size\n",
        "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    # Initialize the model, loss function, and optimizer\n",
        "    model = SimpleNet()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Train the model and get metrics\n",
        "    train_losses, val_losses, train_accuracies, val_accuracies = train_model(\n",
        "        model, train_loader, val_loader, criterion, optimizer, num_epochs\n",
        "    )\n",
        "\n",
        "    # Plot the results\n",
        "    plot_metrics(train_losses, val_losses, train_accuracies, val_accuracies)"
      ]
    },
    {
      "metadata": {
        "id": "b1ea58fbc36a8c4e"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 7: Execute Training\n",
        "\n",
        "Run the main function when the script is executed directly.\n",
        "This is a Python idiom that ensures the training only runs when intended."
      ],
      "id": "b1ea58fbc36a8c4e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e37d4bf",
      "metadata": {
        "id": "0e37d4bf"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}