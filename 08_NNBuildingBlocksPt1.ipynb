{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maleehahassan/HIDA_Into_to_DL/blob/main/08_NNBuildingBlocksPt1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Project: Training a Neuron to Classify Flowers**\n",
        "\n",
        "Objective: Build a complete neural network from scratch (a small one) that can learn to separate two classes of Iris flowers. Let's learn together!"
      ],
      "metadata": {
        "id": "_WMjqvbiHY9O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Overview of the problem**\n",
        "\n",
        "First, we need data. We'll use a small, famous dataset about Iris flowers. Our goal is to train a neuron to differentiate between two species, Iris Setosa and Iris Versicolor, based on their petal length and petal width.\n",
        "\n",
        "This is a classic supervised learning problem: we have input features (X, the petal measurements) and corresponding correct labels (y, the species). Let's load and visualize the data to see what we're working with."
      ],
      "metadata": {
        "id": "tBmRkEjAIA-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "B_QKWOskIT17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining data for flower classification\n",
        "# features (X) = [petal length, petal width]\n",
        "# labels (y) = 0 for Iris-Setosa and 1 for Iris-Versicolor\n",
        "\n",
        "X = np.array([\n",
        "    [1.4, 0.2], [1.3, 0.2], [1.5, 0.2], [1.4, 0.3], # Iris-Setosa\n",
        "    [4.7, 1.4], [4.5, 1.5], [4.9, 1.5], [4.0, 1.3]  # Iris-Versicolor\n",
        "])\n",
        "\n",
        "y = np.array([0,0,0,0,1,1,1,1])\n",
        "\n",
        "# Visualize the data\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:4, 0], X[:4, 1], color='blue', marker='o', label='Iris-Setosa (Class 0)')\n",
        "plt.scatter(X[4:, 0], X[4:, 1], color='red', marker='x', label='Iris-Versicolor (Class 1)')\n",
        "plt.title('Flower Petal Dimensions')\n",
        "plt.xlabel('Petal Length (cm)')\n",
        "plt.ylabel('Petal Width (cm)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aTSHPo9PIcFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2 and 3: A Perceptron with a Sigmoid Activation Function**\n",
        "\n",
        "Our data points look separable by a straight line. A Perceptron is the perfect model for this! It will learn to draw that line.\n",
        "\n",
        "The classic perceptron uses a step function. However, for a smoother learning process, we'll use the Sigmoid function. It squashes the output between 0 and 1, which we can interpret as the probability of a flower being an Iris-Versicolor."
      ],
      "metadata": {
        "id": "cl4Us9wqJlCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the sigmoid activation function\n",
        "def sigmoid(x):\n",
        "    return #your code here\n",
        "\n",
        "# Let's initialize our Perceptron's parameters randomly\n",
        "np.random.seed(42) # for reproducibility\n",
        "\n",
        "# select the number of weights and bias\n",
        "weights = np.random.rand(#your code here)\n",
        "bias = np.random.rand(#your code here)\n",
        "\n",
        "print(f\"Initial Weights: {weights}\")\n",
        "print(f\"Initial Bias: {bias}\")"
      ],
      "metadata": {
        "id": "OD8wjRagKG_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Loss function Mean Squared Error (MSE)**\n",
        "\n",
        "How do we know if our model is doing well? We need a loss function to measure its error. We'll use the Mean Squared Error (MSE), which calculates the average of the squared differences between the true labels (y_true) and our model's predictions (y_pred).\n",
        "\n",
        "The goal of training is to adjust the weights and bias to make this MSE value as low as possible.\n",
        "\n"
      ],
      "metadata": {
        "id": "E9KKTgtyLhDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_squared_error(y_true, y_pred):\n",
        "  return #your code here"
      ],
      "metadata": {
        "id": "nKiHBMlJLy8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Training**\n",
        "\n",
        "Let's make the model learn. Show the data to the model again and again (epoch). in each epoch the model:\n",
        "\n",
        "* Makes a prediction for each flower.\n",
        "\n",
        "* Calculates the loss.\n",
        "\n",
        "* Updates the weights and bias in the direction that reduces the loss (this is a simplified version of gradient descent).\n",
        "\n"
      ],
      "metadata": {
        "id": "HI2n4FisMRgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "learning_rate = 0.1\n",
        "epochs = 100\n",
        "losses = []\n",
        "\n",
        "# The Training Loop\n",
        "for epoch in range(epochs):\n",
        "    # 1. Make predictions\n",
        "    weighted_sum = #your code here\n",
        "    predictions = #your code here\n",
        "\n",
        "    # 2. Calculate the loss\n",
        "    loss = mean_squared_error(y, predictions)\n",
        "    losses.append(loss)\n",
        "\n",
        "    # 3. Calculate the error and update weights/bias\n",
        "    error = #your code here\n",
        "    d_predictions = #your code here\n",
        "\n",
        "    # Calculate gradients\n",
        "    d_weights = #your code here\n",
        "    d_bias = #your code here\n",
        "\n",
        "    # Update parameters\n",
        "    weights += learning_rate * d_weights\n",
        "    bias += learning_rate * d_bias\n",
        "\n",
        "    # Print progress\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "print(\"\\n--- Training Finished ---\")\n",
        "print(f\"Final Weights: {weights}\")\n",
        "print(f\"Final Bias: {bias}\")\n",
        "\n",
        "# Plot the training loss\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(losses)\n",
        "plt.title('Training Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s2V98Er7M97f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6: Visualizing the results**\n",
        "\n",
        "Our model has now learned a set of weights and a bias. Together, these define a line called \"decision boundary\" that separates the two flower classes. Let's plot this line on our original data to see how well our neuron learned!"
      ],
      "metadata": {
        "id": "MBZMcgEMOe8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the original data again\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:4, 0], X[:4, 1], color='blue', marker='o', label='Iris-Setosa (Class 0)')\n",
        "plt.scatter(X[4:, 0], X[4:, 1], color='red', marker='x', label='Iris-Versicolor (Class 1)')\n",
        "\n",
        "# Calculate and plot the decision boundary\n",
        "# The line is where weighted_sum = 0, so np.dot(x, w) + b = 0\n",
        "# w1*x1 + w2*x2 + b = 0  => x2 = (-w1/w2)*x1 - (b/w2)\n",
        "x1_line = np.linspace(1, 5, 100)\n",
        "x2_line = (-weights[0] / weights[1]) * x1_line - (bias / weights[1])\n",
        "\n",
        "plt.plot(x1_line, x2_line, 'k-', label='Learned Decision Boundary')\n",
        "plt.title('Final Classification Result')\n",
        "plt.xlabel('Petal Length (cm)')\n",
        "plt.ylabel('Petal Width (cm)')\n",
        "plt.xlim(1.2, 5.1)\n",
        "plt.ylim(0.1, 1.7)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZfWcDeKOOod6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}